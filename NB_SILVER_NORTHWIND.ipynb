{"cells":[{"cell_type":"markdown","source":["### **Northwind** (Silver Transformation)"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"db12f70f-643b-4870-98de-df966a14470a"},{"cell_type":"code","source":["# Import Requirements\n","import json\n","import re\n","from notebookutils import mssparkutils\n","from pyspark.sql import SparkSession, DataFrame, functions as F, types as T\n","from delta.tables import DeltaTable\n","from typing import Sequence, Optional\n","from delta.tables import DeltaTable\n","from pyspark.sql import DataFrame"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"b9682e0d-bb82-4fe7-bab1-e2e9d87cfc8e","normalized_state":"finished","queued_time":"2025-06-27T17:00:31.4006319Z","session_start_time":null,"execution_start_time":"2025-06-27T17:00:31.4023625Z","execution_finish_time":"2025-06-27T17:00:31.8532299Z","parent_msg_id":"61b86b61-a847-4fd7-b993-73fbfbcf5bd7"},"text/plain":"StatementMeta(, b9682e0d-bb82-4fe7-bab1-e2e9d87cfc8e, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a886d43a-a7d7-4497-b843-059787a2e5fc"},{"cell_type":"markdown","source":["#### **Get Bronze Metadata**\n","Receives Bronze Table Metadata from Copy Activity via _Data Factory Pipeline_"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"705432f0-9a45-49f8-bdb9-c6b56b924f39"},{"cell_type":"code","source":["# Paramameterized Value\n","bronze_metadata = \"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":36,"statement_ids":[36],"state":"finished","livy_statement_state":"available","session_id":"b9682e0d-bb82-4fe7-bab1-e2e9d87cfc8e","normalized_state":"finished","queued_time":"2025-06-27T17:40:32.6705237Z","session_start_time":null,"execution_start_time":"2025-06-27T17:40:32.6722657Z","execution_finish_time":"2025-06-27T17:40:33.3827944Z","parent_msg_id":"acbc24f4-1d5a-4f0b-ab46-2a566059cd96"},"text/plain":"StatementMeta(, b9682e0d-bb82-4fe7-bab1-e2e9d87cfc8e, 36, Finished, Available, Finished)"},"metadata":{}}],"execution_count":34,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"d6384cb8-3cbb-469f-8f71-94f4ac94b016"},{"cell_type":"code","source":["def clean_json(payload: str) -> list:\n","    \"\"\"Parse & un-escape incoming JSON array\"\"\"\n","    raw = payload.strip()\n","    if raw.startswith('\"') and raw.endswith('\"'):\n","        raw = raw[1:-1].replace('\\\\\"', '\"')\n","    records = json.loads(raw)\n","    return records\n","\n","# STEP 1: Parse & Normalize Incoming Metadata\n","parsed = clean_json(bronze_metadata)\n","\n","# STEP 2: Unwrap nested BronzeMetadata\n","records = [entry[\"BronzeMetadata\"] for entry in parsed if \"BronzeMetadata\" in entry]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":-1,"statement_ids":[],"state":"session_error","livy_statement_state":null,"session_id":"35386cd5-7eb1-4640-8426-f936f171b8bc","normalized_state":"session_error","queued_time":"2025-06-28T10:34:01.2394808Z","session_start_time":"2025-06-28T10:34:01.2410613Z","execution_start_time":null,"execution_finish_time":"2025-06-28T10:34:11.049243Z","parent_msg_id":"e840497c-0af8-430a-9a30-e4107a781b49"},"text/plain":"StatementMeta(, 35386cd5-7eb1-4640-8426-f936f171b8bc, -1, SessionError, , SessionError)"},"metadata":{}}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b36c0cdc-6720-43aa-af90-6eec8e1f80df"},{"cell_type":"markdown","source":["#### **Data Transformation**\n","Custom Cleaning Functions for Each Table"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fa483f69-d4c9-420a-90a7-c5bb82a28d16"},{"cell_type":"code","source":["def clean_categories(raw_df):\n","    # Capitalize the first character in column: 'Description'\n","    raw_df = raw_df.withColumn('Description', F.initcap(F.col('Description')))\n","    # Drop column: 'Picture'\n","    raw_df = raw_df.drop('Picture')\n","    return raw_df\n","\n","def clean_customers(raw_df):\n","    # Capitalize the first character in column: 'CompanyName'\n","    raw_df = raw_df.withColumn('CompanyName', F.initcap(F.col('CompanyName')))\n","    # Capitalize the first character in column: 'Address'\n","    raw_df = raw_df.withColumn('Address', F.initcap(F.col('Address')))\n","    return raw_df\n","\n","def clean_employees(raw_df):\n","    # Change column type to datetime64[ns] for column: 'BirthDate'\n","    raw_df = raw_df.withColumn('BirthDate', raw_df['BirthDate'].cast(T.TimestampType()))\n","    # Change column type to datetime64[ns] for column: 'HireDate'\n","    raw_df = raw_df.withColumn('HireDate', raw_df['HireDate'].cast(T.TimestampType()))\n","    # Rename column 'ReportsTo' to 'Manager'\n","    raw_df = raw_df.withColumnRenamed('ReportsTo', 'Manager')\n","    # Drop columns: 'PhotoPath', 'Photo'\n","    raw_df = raw_df.drop('PhotoPath', 'Photo')\n","    return raw_df\n","\n","def clean_employee_territories(raw_df):\n","    # Change column type to int64 for column: 'TerritoryID'\n","    raw_df = raw_df.withColumn('TerritoryID', raw_df['TerritoryID'].cast(T.LongType()))\n","    return raw_df\n","\n","def clean_order_details(raw_df):\n","    # Change column type to float64 for column: 'UnitPrice'\n","    raw_df = raw_df.withColumn('UnitPrice', raw_df['UnitPrice'].cast(T.DoubleType()))\n","    # Round column 'UnitPrice' (Number of decimals: 2)\n","    raw_df = raw_df.withColumn('UnitPrice', F.round(F.col('UnitPrice'), 2))\n","    return raw_df\n","\n","def clean_orders(raw_df):\n","    # Change column type to datetime64[ns] for columns: 'OrderDate', 'RequiredDate', 'ShippedDate'\n","    raw_df = raw_df.withColumn('OrderDate', raw_df['OrderDate'].cast(T.TimestampType()))\n","    raw_df = raw_df.withColumn('RequiredDate', raw_df['RequiredDate'].cast(T.TimestampType()))\n","    raw_df = raw_df.withColumn('ShippedDate', raw_df['ShippedDate'].cast(T.TimestampType()))\n","    # Rename column 'Freight' to 'FreightCosts'\n","    raw_df = raw_df.withColumnRenamed('Freight', 'FreightCosts')\n","    # Change column type to float64 for column: 'FreightCosts'\n","    raw_df = raw_df.withColumn('FreightCosts', raw_df['FreightCosts'].cast(T.DoubleType()))\n","    # Round column 'FreightCosts' (Number of decimals: 2)\n","    raw_df = raw_df.withColumn('FreightCosts', F.round(F.col('FreightCosts'), 2))\n","    # Capitalize the first character in column: 'ShipName'\n","    raw_df = raw_df.withColumn('ShipName', F.initcap(F.col('ShipName')))\n","    # Capitalize the first character in column: 'ShipAddress'\n","    raw_df = raw_df.withColumn('ShipAddress', F.initcap(F.col('ShipAddress')))\n","    return raw_df\n","\n","def clean_products(raw_df):\n","    # Capitalize the first character in column: 'ProductName'\n","    raw_df = raw_df.withColumn('ProductName', F.initcap(F.col('ProductName')))\n","    # Change column type to float64 for column: 'UnitPrice'\n","    raw_df = raw_df.withColumn('UnitPrice', raw_df['UnitPrice'].cast(T.DoubleType()))\n","    return raw_df\n","\n","def clean_region(raw_df):\n","    return raw_df\n","\n","def clean_shippers(raw_df):\n","    return raw_df\n","\n","def clean_suppliers(raw_df):\n","    # Capitalize the first character in column: 'Address'\n","    raw_df = raw_df.withColumn('Address', F.initcap(F.col('Address')))\n","    # Drop column: 'HomePage'\n","    raw_df = raw_df.drop('HomePage')\n","    return raw_df\n","\n","def clean_territories(raw_df):\n","    # Change column type to int64 for column: 'TerritoryID'\n","    raw_df = raw_df.withColumn('TerritoryID', raw_df['TerritoryID'].cast(T.LongType()))\n","    return raw_df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"b9682e0d-bb82-4fe7-bab1-e2e9d87cfc8e","normalized_state":"finished","queued_time":"2025-06-27T17:00:44.1293014Z","session_start_time":null,"execution_start_time":"2025-06-27T17:00:44.1311871Z","execution_finish_time":"2025-06-27T17:00:44.5519031Z","parent_msg_id":"53e619e4-ebfb-4799-9285-63d4f4bd5e88"},"text/plain":"StatementMeta(, b9682e0d-bb82-4fe7-bab1-e2e9d87cfc8e, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0f6d2f02-4cf9-4a38-a019-8ec5959a0dbc"},{"cell_type":"code","source":["cleaning_functions = {\n","    \"Categories\": clean_categories,\n","    \"Customers\": clean_customers,\n","    \"Employees\": clean_employees,\n","    \"EmployeeTerritories\": clean_employee_territories,\n","    \"OrderDetails\": clean_order_details,\n","    \"Orders\": clean_orders,\n","    \"Products\": clean_products,\n","    \"Region\": clean_region,\n","    \"Shippers\": clean_shippers,\n","    \"Suppliers\": clean_suppliers,\n","    \"Territories\": clean_territories\n","}\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":37,"statement_ids":[37],"state":"finished","livy_statement_state":"available","session_id":"b9682e0d-bb82-4fe7-bab1-e2e9d87cfc8e","normalized_state":"finished","queued_time":"2025-06-27T18:06:30.6272093Z","session_start_time":null,"execution_start_time":"2025-06-27T18:06:30.6288947Z","execution_finish_time":"2025-06-27T18:06:31.1905417Z","parent_msg_id":"ed5e903b-e9b6-44e4-b15e-af941fd09c69"},"text/plain":"StatementMeta(, b9682e0d-bb82-4fe7-bab1-e2e9d87cfc8e, 37, Finished, Available, Finished)"},"metadata":{}}],"execution_count":35,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dec44602-b25d-4edb-8a38-7c70d70549bc"},{"cell_type":"markdown","source":["#### **Data Loading**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ff19f0ea-b0d8-4ec8-9fd5-aa9623cb6e23"},{"cell_type":"code","source":["def upsert(dataframe: DataFrame, table_name: str, key_cols: list[str], partition_cols: list[str] = None, schema_name: str = None):\n","    \"\"\"\n","    Upsert into a Delta table in Microsoft Fabric Lakehouse.\n","\n","    If schema_name is provided, uses schema.table_name format and ensures the schema exists.\n","    \"\"\"\n","\n","    # Compose full name and path\n","    if schema_name:\n","        full_table_name = f\"{schema_name}.{table_name}\"\n","        table_path = f\"Tables/{schema_name}/{table_name}\"\n","        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {schema_name}\")\n","    else:\n","        full_table_name = table_name\n","        table_path = f\"Tables/{table_name}\"\n","\n","    # Drop Duplicates\n","    dataframe = dataframe.dropDuplicates(key_cols)\n","\n","    if DeltaTable.isDeltaTable(spark, table_path):\n","        delta_table = DeltaTable.forPath(spark, table_path)\n","        merge_condition = \" AND \".join([f\"target.{col} = source.{col}\" for col in key_cols])\n","\n","        delta_table.alias(\"target\") \\\n","            .merge(\n","                source=dataframe.alias(\"source\"),\n","                condition=merge_condition\n","            ) \\\n","            .whenMatchedUpdateAll() \\\n","            .whenNotMatchedInsertAll() \\\n","            .execute()\n","\n","        print(f\"✅ Upserted into existing table: {full_table_name}\")\n","    else:\n","        writer = dataframe.write.format(\"delta\").mode(\"overwrite\")\n","        if partition_cols:\n","            writer = writer.partitionBy(*partition_cols)\n","\n","        writer.saveAsTable(full_table_name)\n","        print(f\"✅ Created new table: {full_table_name}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":27,"statement_ids":[27],"state":"finished","livy_statement_state":"available","session_id":"b9682e0d-bb82-4fe7-bab1-e2e9d87cfc8e","normalized_state":"finished","queued_time":"2025-06-27T17:23:28.4731019Z","session_start_time":null,"execution_start_time":"2025-06-27T17:23:28.4748131Z","execution_finish_time":"2025-06-27T17:23:28.9105048Z","parent_msg_id":"71c0b6ff-23d0-4939-96e7-3639968a847e"},"text/plain":"StatementMeta(, b9682e0d-bb82-4fe7-bab1-e2e9d87cfc8e, 27, Finished, Available, Finished)"},"metadata":{}}],"execution_count":25,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2afd89a7-78f6-4a2b-ab84-a8d07bdbf1ea"},{"cell_type":"markdown","source":["#### **Silver Ingestion**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ec7ea9fd-478e-43a0-a140-b4f9c9eaee07"},{"cell_type":"code","source":["for rec in records:\n","\n","    if rec.get(\"BronzeStatus\") != \"Success\":\n","        rec[\"SilverStatus\"] = \"Skipped\"\n","        continue\n","\n","    try:\n","        # Metadata\n","        DatabaseName = rec[\"DatabaseName\"].replace(\"_\", \"\").replace(\"-\", \"\")\n","        TableName = rec[\"TableName\"].strip().replace(\" \", \"\") # Remove Space in \"Order Details\"\n","        TableSchema = rec[\"TableSchema\"]\n","        KeyColumn = rec[\"KeyColumn\"]\n","        DataDomain = str(rec[\"DataDomain\"]).lower()\n","\n","        # Read raw file\n","        df = spark.read.parquet(f\"Files/{rec['BronzeFolderPath']}\")\n","        df = df.dropDuplicates(rec[\"KeyColumn\"].split(\"|\")) \n","\n","        # ✅ Apply matching cleaning function\n","        cleaning_fn = cleaning_functions.get(TableName)\n","        if cleaning_fn:\n","            df = cleaning_fn(df)  # Apply it only if exists\n","        else:\n","            print(f\"⚠️ No cleaning function found for {TableName}, using raw DataFrame.\")\n","\n","        # Partition logic\n","        partition_cols = None\n","        if rec.get(\"PartitionColumn\"):\n","            part_col = rec[\"PartitionColumn\"]\n","            part_type = str(rec.get(\"PartitionType\", \"\")).strip().lower()\n","\n","            if part_type == \"date\":\n","                df = df.withColumn(part_col, F.to_date(F.col(part_col)))\n","                df = df.withColumn(\"Year\", F.year(F.col(part_col)))\n","                df = df.withColumn(\"Month\", F.month(F.col(part_col)))\n","                df = df.withColumn(\"Day\", F.dayofmonth(F.col(part_col)))\n","                partition_cols = [\"Year\", \"Month\", \"Day\"]\n","\n","            elif part_type == \"categorical\":\n","                partition_cols = [part_col]\n","\n","        # ✅ Upsert to Silver\n","        upsert(\n","            dataframe=df,\n","            table_name=TableName,\n","            key_cols=KeyColumn.split(\"|\"),\n","            partition_cols=partition_cols,\n","            schema_name=DataDomain \n","        )\n","\n","        rec[\"SilverStatus\"] = \"Success\"\n","        rec[\"SilverFolderPath\"] = f\"Tables/{DataDomain}/{TableName}\"\n","\n","    except Exception as e:\n","        rec[\"SilverStatus\"] = f\"Failed: {str(e)[:500]}\"\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e508c520-24d9-42da-a037-1a0dbf210935"},{"cell_type":"markdown","source":["#### **Notebook Return Output**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9f106566-dd4e-4bd1-bdd4-54f0fa7824eb"},{"cell_type":"code","source":["# Optional: pretty-print the output\n","print(\"Returning payload →\", json.dumps(records, indent=2))\n","\n","# Return full JSON for pipeline/logging use\n","mssparkutils.notebook.exit(json.dumps(records))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"44f5f6b6-957b-4eba-a2b9-20b8645d526f"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"708e5bee-37a5-49d1-95e9-da893771fbb6"}],"default_lakehouse":"708e5bee-37a5-49d1-95e9-da893771fbb6","default_lakehouse_name":"LH_SILVER","default_lakehouse_workspace_id":"63133ed5-ae8c-44aa-ac94-7a2c7f5d65cc"}}},"nbformat":4,"nbformat_minor":5}